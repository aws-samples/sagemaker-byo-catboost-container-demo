{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a custom container and Estimator to run Catboost on SageMaker\n",
    "\n",
    "In this notebook, we use the SageMaker Training Toolkit (https://github.com/aws/sagemaker-training-toolkit) to create a SageMaker-compatible docker image to run python scripts using the Catboost algorithm library. We also show how to create a custom SageMaker training `Estimator` from the SageMaker `Framework` class (https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.Framework)\n",
    "\n",
    "CatBoost is a high-performance open source library for gradient boosting on decision trees. You can learn more about it at the following links:\n",
    "* https://tech.yandex.com/catboost/\n",
    "* https://catboost.ai/\n",
    "* https://github.com/catboost/catboost\n",
    "\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "We use the Boston Housing dataset, present in Scikit-Learn: https://scikit-learn.org/stable/datasets/index.html#boston-dataset\n",
    "\n",
    "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic prices and the demand for clean air', J. Environ. Economics & Management, vol.5, 81-102, 1978. Used in Belsley, Kuh & Welsch, 'Regression diagnostics ...', Wiley, 1980. N.B. Various transformations are used in the table on pages 244-261 of the latter.\n",
    "\n",
    "The Boston house-price data has been used in many machine learning papers that address regression problems.\n",
    "\n",
    "References\n",
    "\n",
    " * Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
    " * Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
    "\n",
    "**This sample is provided for demonstration purposes, make sure to conduct appropriate testing if derivating this code for your own use-cases!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Container creation and upload to Amazon ECR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a SageMaker-compatible Catboost container\n",
    "We derive our dockerfile from the SageMaker Scikit-Learn dockerfile https://github.com/aws/sagemaker-scikit-learn-container/blob/master/docker/0.20.0/base/Dockerfile.cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "FROM ubuntu:16.04\n",
    "\n",
    "RUN apt-get update && \\\n",
    "    apt-get -y install build-essential libatlas-dev git wget curl nginx jq libatlas3-base\n",
    "\n",
    "RUN curl -LO http://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh && \\\n",
    "    bash Miniconda3-latest-Linux-x86_64.sh -bfp /miniconda3 && \\\n",
    "    rm Miniconda3-latest-Linux-x86_64.sh\n",
    "\n",
    "ENV PATH=/miniconda3/bin:${PATH}\n",
    "        \n",
    "RUN apt-get update && apt-get install -y python-pip && pip install sagemaker-training catboost\n",
    "\n",
    "ENV PYTHONDONTWRITEBYTECODE=1 PYTHONUNBUFFERED=1 PYTHONIOENCODING=UTF-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending the container to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "ecr_namespace = 'sagemaker-training-containers/'\n",
    "prefix = 'catboost-image'\n",
    "\n",
    "ecr_repository_name = ecr_namespace + prefix\n",
    "account_id = role.split(':')[4]\n",
    "region = boto3.Session().region_name\n",
    "sess = sagemaker.session.Session()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "print('Account: {}'.format(account_id))\n",
    "print('Region: {}'.format(region))\n",
    "print('Role: {}'.format(role))\n",
    "print('S3 Bucket: {}'.format(bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile build_and_push.sh\n",
    "\n",
    "ACCOUNT_ID=$1\n",
    "REGION=$2\n",
    "REPO_NAME=$3\n",
    "\n",
    "if [[ $REGION =~ ^cn.* ]]\n",
    "then\n",
    "    FULLNAME=\"${ACCOUNT_ID}.dkr.ecr.${REGION}.amazonaws.com.cn/${REPO_NAME}:latest\"\n",
    "else\n",
    "    FULLNAME=\"${ACCOUNT_ID}.dkr.ecr.${REGION}.amazonaws.com/${REPO_NAME}:latest\"\n",
    "fi\n",
    "\n",
    "echo $FULLNAME\n",
    "\n",
    "docker build -f Dockerfile -t $REPO_NAME .\n",
    "\n",
    "docker tag $REPO_NAME $FULLNAME\n",
    "\n",
    "$(aws ecr get-login --no-include-email --registry-ids $ACCOUNT_ID)\n",
    "\n",
    "aws ecr describe-repositories --repository-names $REPO_NAME || aws ecr create-repository --repository-name $REPO_NAME\n",
    "\n",
    "docker push $FULLNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! bash build_and_push.sh $account_id $region $ecr_repository_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cn' in region:\n",
    "    container_image_uri = '{0}.dkr.ecr.{1}.amazonaws.com.cn/{2}:latest'.format(account_id, region, ecr_repository_name)\n",
    "else:\n",
    "    container_image_uri = '{0}.dkr.ecr.{1}.amazonaws.com/{2}:latest'.format(account_id, region, ecr_repository_name)\n",
    "print('ECR container ARN: {}'.format(container_image_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The docker image is now pushed to ECR and is ready for consumption! In the next section, we go in the shoes of an ML practitioner that develops a Catboost model and runs it remotely on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: local ML development and remote training job with Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We install catboost locally for local development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing\n",
    "We use pandas to process a small local dataset into a training and testing piece.\n",
    "\n",
    "We could also design code that loads all the data and runs cross-validation within the script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the Boston housing dataset \n",
    "data = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.data, data.target, test_size=0.25, random_state=42)\n",
    "\n",
    "trainX = pd.DataFrame(X_train, columns=data.feature_names)\n",
    "trainX['target'] = y_train\n",
    "\n",
    "testX = pd.DataFrame(X_test, columns=data.feature_names)\n",
    "testX['target'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_train = 'boston_train.csv'\n",
    "local_test = 'boston_test.csv'\n",
    "\n",
    "trainX.to_csv(local_train)\n",
    "testX.to_csv(local_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send data to S3. SageMaker will take training data from S3\n",
    "train_location = sess.upload_data(\n",
    "    path=local_train, \n",
    "    bucket=bucket,\n",
    "    key_prefix='catboost')\n",
    "\n",
    "test_location = sess.upload_data(\n",
    "    path=local_test, \n",
    "    bucket=bucket,\n",
    "    key_prefix='catboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing a local training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile catboost_training.py\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "if __name__ =='__main__':\n",
    "\n",
    "    print('extracting arguments')\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n",
    "    parser.add_argument('--train-file', type=str, default='boston_train.csv')\n",
    "    parser.add_argument('--test-file', type=str, default='boston_test.csv')\n",
    "    parser.add_argument('--model-name', type=str, default='catboost_model.dump')\n",
    "    parser.add_argument('--features', type=str)  # in this script we ask user to explicitly name features\n",
    "    parser.add_argument('--target', type=str) # in this script we ask user to explicitly name the target\n",
    "    \n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    logging.info('reading data')\n",
    "    train_df = pd.read_csv(os.path.join(args.train, args.train_file))\n",
    "    test_df = pd.read_csv(os.path.join(args.test, args.test_file))\n",
    "\n",
    "    logging.info('building training and testing datasets')\n",
    "    X_train = train_df[args.features.split()]\n",
    "    X_test = test_df[args.features.split()]\n",
    "    y_train = train_df[args.target]\n",
    "    y_test = test_df[args.target]\n",
    "        \n",
    "    # define and train model\n",
    "    model = CatBoostRegressor()\n",
    "    \n",
    "    model.fit(X_train, y_train, eval_set=(X_test, y_test), logging_level='Silent') \n",
    "    \n",
    "    # print abs error\n",
    "    logging.info('validating model')\n",
    "    abs_err = np.abs(model.predict(X_test) - y_test)\n",
    "\n",
    "    # print couple perf metrics\n",
    "    for q in [10, 50, 90]:\n",
    "        logging.info('AE-at-' + str(q) + 'th-percentile: '\n",
    "              + str(np.percentile(a=abs_err, q=q)))\n",
    "    \n",
    "    # persist model\n",
    "    path = os.path.join(args.model_dir, args.model_name)\n",
    "    logging.info('saving to {}'.format(path))\n",
    "    model.save_model(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our script locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local test\n",
    "! python catboost_training.py \\\n",
    "    --train ./ \\\n",
    "    --test ./ \\\n",
    "    --model-dir ./ \\\n",
    "    --features 'CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT' \\\n",
    "    --target target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remote training in SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Launch a SageMaker training job from code uploaded to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that option, we first need to send code to S3. This could also be done automatically by a build system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first compress the code and send to S3\n",
    "program = 'catboost_training.py'\n",
    "source = 'source.tar.gz'\n",
    "project = 'catboost'\n",
    "\n",
    "tar = tarfile.open(source, 'w:gz')\n",
    "tar.add(program)\n",
    "tar.close()\n",
    "\n",
    "submit_dir = sess.upload_data(\n",
    "    path=source, \n",
    "    bucket=bucket,\n",
    "    key_prefix=project+ '/' + source)\n",
    "\n",
    "print(submit_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then launch a training job with the `Estimator` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "output_path = 's3://' + bucket + '/' + project + '/' + 'training_jobs'\n",
    "\n",
    "estimator = Estimator(image_uri=container_image_uri,\n",
    "                      role=role,\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type='ml.m5.xlarge',\n",
    "                      output_path=output_path,\n",
    "                      hyperparameters={'sagemaker_program': program,\n",
    "                                       'sagemaker_submit_directory': submit_dir,\n",
    "                                       'features': 'CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT',\n",
    "                                       'target': 'target'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit({'train':train_location, 'test': test_location}, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Launch a SageMaker training job using a custom Estimator and a local training script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it even faster to iterate between local development and remote training in SageMaker, we can create a custom `Estimator` by extending the [Framework](https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.Framework) class from the SageMaker SDK. This will perform the code compression and S3 upload for us:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Framework\n",
    "\n",
    "class CatBoostEstimator(Framework):\n",
    "    def __init__(\n",
    "        self,\n",
    "        entry_point,\n",
    "        source_dir=None,\n",
    "        hyperparameters=None,\n",
    "        py_version=\"py3\",\n",
    "        framework_version=None,\n",
    "        image_uri=None,\n",
    "        distributions=None,\n",
    "        **kwargs):\n",
    "        \n",
    "        super(CatBoostEstimator, self).__init__(\n",
    "            entry_point, source_dir, hyperparameters, image_uri=image_uri, **kwargs)\n",
    "        \n",
    "        self.framework_version = framework_version\n",
    "        self.py_version = py_version\n",
    "    \n",
    "    \n",
    "    def _configure_distribution(self, distributions):\n",
    "        return\n",
    "    \n",
    "    def create_model(\n",
    "        self,\n",
    "        model_server_workers=None,\n",
    "        role=None,\n",
    "        vpc_config_override=None,\n",
    "        entry_point=None,\n",
    "        source_dir=None,\n",
    "        dependencies=None,\n",
    "        image_uri=None,\n",
    "        **kwargs):\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost = CatBoostEstimator(\n",
    "    image_uri=container_image_uri,\n",
    "    role=role,\n",
    "    entry_point='catboost_training.py',\n",
    "    output_path=output_path,\n",
    "    train_instance_count=1, \n",
    "    train_instance_type='ml.m5.xlarge',\n",
    "    hyperparameters={'features': 'CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT',\n",
    "                     'target': 'target'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost.fit({'train':train_location, 'test': test_location}, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can accelerate our use of Catboost with all the nice SageMaker features! including:\n",
    " 1. Bayesian tuning of hyperparameters\n",
    " 1. Remote persistance of metadata, hyperparameter, model artifacts, metrics and logs\n",
    " 1. Hardware scaling and GPU use\n",
    " 1. Connection to large S3 data sources"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
